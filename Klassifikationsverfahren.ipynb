{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12200cf2",
   "metadata": {},
   "source": [
    "# Klassifikationsverfahren\n",
    "In diesem Jupyter Notebook werden die in der Vorlesung besprochenen Klassifikationsverfahren an Hand einiger (synthetischer) [Datensätze](https://github.com/MarkEich96/Maschinelles-Lernen-SoSe-2024) erprobt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b33400",
   "metadata": {},
   "source": [
    "## Vorbereitung\n",
    "Um schnelle Matrixmultiplikationen durchzuführen und die Daten aufzutragen, werden die Pakete `numpy` und `matplotlib` verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42d280b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7277c0a4",
   "metadata": {},
   "source": [
    "## Lineare Separation\n",
    "Im Datensatz `classification_data01.dat` sind Datenpunkt im zwei dimensionalen Raum $\\mathbb{R}^2$ (um genauer zu sein aus der Menge $[0, 1]\\times[0, 1]$) mit den Klassenlabels $y\\in\\{0, 1\\}$ aufgetragen. \n",
    "\n",
    "__Aufgabe__: Lade die Daten und trage diese auf. Überzeuge Dich davon, dass sie linear separierbar sind. Kannst Du erahnen, wie groß der Versionsraum der konsistenten und vollständigen Hypothesen ist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6110cf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Einlesen der Daten\n",
    "x1, x2, y = \n",
    "\n",
    "# Anlegen eines Plots\n",
    "plt.figure(figsize = (5, 5))\n",
    "\n",
    "# Auftragen der Daten \n",
    "\n",
    "\n",
    "# Zurechtmachen der Achsen\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eab56b",
   "metadata": {},
   "source": [
    "Durch das Anlegen eines geraden Objekts am Bildschirm lässt sich grob einschätzen, dass die Steigung im Intervall $[-1, -0.05]$ und der y-Achsenabschnitt ungefähr im Bereich $[0.5, 0.9]$ liegt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088a1957",
   "metadata": {},
   "source": [
    "In der Vorlesung wurde besprochen, dass als Modell die Hypothese\n",
    "$$\n",
    "h_{\\vec{w}}(\\vec{x})=\\mathrm{sig}(\\vec{w}\\cdot\\vec{x}+w_0)=\\frac{1}{1+\\mathrm{exp}(-(\\vec{w}\\cdot\\vec{x}+w_0))}\n",
    "$$\n",
    "verwendet werden kann und sich daraus das empirische Risiko\n",
    "$$\n",
    "\\hat{R}=\\frac{1}{N}\\sum_{i=1}^N\\mathrm{ln}(1+\\mathrm{exp}((-1)^{y_i}(\\underline{X}'\\vec{w})_i))\n",
    "$$\n",
    "mit der erweiterten Datenmatrix\n",
    "$$\n",
    "\\underline{X}'=\\begin{pmatrix}\n",
    " 1 & x_{11} & x_{12} & \\cdots & x_{1d}\\\\\n",
    " 1 & x_{21} & x_{22} & \\cdots & x_{2d}\\\\\n",
    " \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    " 1 & x_{N1} & x_{N2} & \\cdots & x_{Nd}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "ergibt. Der Gradient war durch \n",
    "$$\n",
    "\\frac{\\partial \\hat{R}}{\\partial w_j}=\\frac{1}{N}\\sum_{i=1}^N\\frac{(-1)^{y_i}\\underline{X}'_{ij}}{1+\\mathrm{exp}(-(-1)^{y_i}(\\underline{X}'\\vec{w})_i)}\n",
    "$$\n",
    "gegeben.\n",
    "\n",
    "__Aufgabe__: Verwende das Gradientenabstiegsverfahren, um die Gewichte $\\hat{\\vec{w}}$ zu bestimmen. Bestimme aus diesen die Steigung und den y-Achsenabschnitt der Trenngeraden. Ermittle dann das empirische Riskio (also den Trainingsfehler). Trage die Ergebnisse mittels des Kontur-Plots im untenstehenden Code-Gerüst auf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16232ef6",
   "metadata": {},
   "source": [
    "#### Gradientenabstiegsverfahren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e11107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aufbauen der erweiterten Datenmatrix\n",
    "N = len(y)\n",
    "d = 2\n",
    "X = np.zeros(shape = (N, d+1))\n",
    "for n in range(N):\n",
    "    X[n] = \n",
    "    \n",
    "# Errechnen des Vorzeichenvektors\n",
    "z = (-1)**y\n",
    "    \n",
    "# Festlegen von Hyperparametern\n",
    "learning_rate = \n",
    "epochen = \n",
    "\n",
    "# Initialisieren der Gewichte\n",
    "w = \n",
    "\n",
    "# Gradientenabstieg\n",
    "for n in range(epochen):\n",
    "    grad_vec = np.zeros(d+1)\n",
    "    for j in range(d):\n",
    "        grad_vec[j] = \n",
    "    w = w-learning_rate*grad_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16e72b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Die Steigung liegt bei m = {-w[1]/w[2]:.2f}, während der y-Achsenabschnitt durch b = {-w[0]/w[2]:.2f} gegeben ist.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b98e964",
   "metadata": {},
   "source": [
    "#### Auftragen der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b2cf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anlegen eines Plots\n",
    "plt.figure(figsize = (5, 5))\n",
    "\n",
    "# Auftragen der Daten \n",
    "\n",
    "\n",
    "# Erstellen eines Meshgrids für den Contour-Plot\n",
    "X1, X2 = np.meshgrid(np.linspace(0, 1, 100), np.linspace(0, 1, 100))\n",
    "\n",
    "# Erstellen eines Contour-Plots mit erfüllen der Gleichung bei Contour == 0\n",
    "plt.contour(X1, X2, w[0]+w[1]*X1+w[2]*X2,  [0])\n",
    "\n",
    "# Zurechtmachen der Achsen\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f755fac8",
   "metadata": {},
   "source": [
    "#### Berechnen des Trainingsfehlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b105b868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementieren der Formel für den Testfehler\n",
    "R = \n",
    "print(f'Der Testfehler ist durch {R:.4f} gegeben.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa577336",
   "metadata": {},
   "source": [
    "## Validierung\n",
    "Im Datensatz `classification_data02.dat` ist ein Testdatensatz für das Modell hinterlegt.\n",
    "\n",
    "__Aufgabe__: Lade den Testdatensatz und trage ihn zusammen mit dem Entscheidungsmodell auf. Ist die gefundene Hypothese mit dem Testdatensatz konsistent? Bestimme dann den Test- und daraus den Verallgemeinerungsfehler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf0a2bb",
   "metadata": {},
   "source": [
    "#### Auftragen des Validierungsdatensatzes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e42713b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Laden des Validierungsdatensatzes\n",
    "x1_test, x2_test, y_test = \n",
    "\n",
    "# Anlegen eines Plots\n",
    "plt.figure(figsize = (5, 5))\n",
    "\n",
    "# Auftragen der Daten \n",
    "\n",
    "\n",
    "# Erstellen eines Meshgrids für den Contour-Plot\n",
    "X1, X2 = np.meshgrid(np.linspace(0, 1, 100), np.linspace(0, 1, 100))\n",
    "\n",
    "# Erstellen eines Contour-Plots mit erfüllen der Gleichung bei Contour == 0\n",
    "plt.contour(X1, X2, w[0]+w[1]*X1+w[2]*X2,  [0])\n",
    "\n",
    "# Zurechtmachen der Achsen\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4918bdb3",
   "metadata": {},
   "source": [
    "Mit den Hyperparametern ``eta = 10**(-3)`` und ``epochen = 1000`` und den initialisierten Gewichten ``w = [1, 1, 1]`` tritt ein Datenpunkt auf, der mit der gefundenen Hypothese nicht konsistent ist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e132eb",
   "metadata": {},
   "source": [
    "#### Bestimmen der Fehler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf3730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bestimmen der Datenmatrix für die Validierungsdaten\n",
    "N = len(y_test)\n",
    "d = 2\n",
    "X_test = np.zeros(shape = (N, d+1))\n",
    "for n in range(N):\n",
    "    X_test[n] = \n",
    "    \n",
    "# Errechnen des Vorzeichenvektors\n",
    "z_test = (-1)**y_test\n",
    "\n",
    "# Errechnen des Validierungsfehlers\n",
    "R_test = \n",
    "print(f'Der Testfehler ist durch {R_test:.4f} gegeben.')\n",
    "print(f'Damit liegt der Verallgemeinerungsfehler bei {R_test-R:.4f} gegeben.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f66329",
   "metadata": {},
   "source": [
    "## Nicht lineare Separation\n",
    "Im Datensatz `classification_data03.dat` sind $N=100$ Datenpunkte $(x_1, x_2)^T\\in\\mathbb{R}^2$ mit den zugeörigen Klassenlabels $y\\in\\{0, 1\\}$ zu finden. \n",
    "\n",
    "__Aufgabe__: Ergänze das nachstehende Code-Gerüst, um Dich davon zu überzeugen, dass die Daten nicht linear separierbar sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a1b647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laden aller Daten in einen einzigen Array\n",
    "Data = np.loadtxt('classification_data03.dat')\n",
    "\n",
    "# Definieren einer Funktion, die aus dem Array aller Daten die Anteile x1, x2 und y herauszieht\n",
    "# Die Klassenlabels werden direkt in Vorzeichen umgewandelt, da diese für die Berechnungen wichtiger sind\n",
    "def data_to_structure(data):\n",
    "    x1, x2, y = \n",
    "    z = \n",
    "    return x1, x2, z\n",
    "\n",
    "# Formatieren der Daten mit obiger Funktion\n",
    "x1, x2, z = data_to_structure(Data)\n",
    "\n",
    "### Auftragen der Daten ###\n",
    "# Anlegen eines Plots\n",
    "plt.figure(figsize = (5, 5))\n",
    "\n",
    "# Auftragen der Daten \n",
    "\n",
    "\n",
    "# Zurechtmachen der Achsen\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5acdf57",
   "metadata": {},
   "source": [
    "In der Vorlesung haben wir auch darüber geredet, dass wir einen Datensatz in einen Trainingsdatensatz, einen Validierungsdatensatz und einen Testdatensatz aufteilen können. Dies wollen wir am vorliegenden Datensatz ausprobieren. Dazu kann die Numpy-Funktion `np.random.shuffle()` verwendet werden, die einen Array zufällig durchmischt. Mit den Aufrufen `a[n:m]` werden alle Array-Elemente von `n` beginnend bis `m-1` aus dem Array entnommen. \n",
    "\n",
    "__Aufgabe__: Teile den Datensatz `Data` in einen Trainings- (70%), Validierungs- (20%) und Testdatensatz (10%) auf und formatiere jeden dieser Datensätze so, dass am Ende jeweils die Arrays `x1`, `x2` und `z` für jeden Datensatz vorliegen. Trage diese drei Datensätze nebeneinander auf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3078ffd9",
   "metadata": {},
   "source": [
    "#### Zerlegen und Formatieren der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5459db5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zufälliges durchmischen des Datensatzes\n",
    "\n",
    "\n",
    "# Zerlegen in drei Arrays mit 70 % der Daten, 20 % der Daten und 10 % der Daten.\n",
    "Data_train = \n",
    "Data_val = \n",
    "Data_test = \n",
    "\n",
    "# Formatieren der Daten mit obiger Funktion\n",
    "x1_train, x2_train, z_train = data_to_structure(Data_train)\n",
    "x1_val, x2_val, z_val = data_to_structure(Data_val)\n",
    "x1_test, x2_test, z_test = data_to_structure(Data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73aed353",
   "metadata": {},
   "source": [
    "#### Auftragen der Datensätze nebeneinander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cec380e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Erstellen eines großen Plots\n",
    "plt.figure(figsize = (15, 5))\n",
    "\n",
    "# Auftragen des Trainingsdatensatzes \n",
    "plt.subplot(131)\n",
    "\n",
    "\n",
    "\n",
    "plt.xlim(-1.2, 1.2)\n",
    "plt.ylim(-.2, 1.6)\n",
    "plt.grid(True, color = 'gray', linestyle = '--')\n",
    "\n",
    "# Auftragen des Validierungsdatensatzes\n",
    "plt.subplot(132)\n",
    "\n",
    "\n",
    "\n",
    "plt.xlim(-1.2, 1.2)\n",
    "plt.ylim(-.2, 1.6)\n",
    "plt.grid(True, color = 'gray', linestyle = '--')\n",
    "\n",
    "# Auftragen des Testdatensatzes\n",
    "plt.subplot(133)\n",
    "\n",
    "\n",
    "\n",
    "plt.xlim(-1.2, 1.2)\n",
    "plt.ylim(-.2, 1.6)\n",
    "plt.grid(True, color = 'gray', linestyle = '--')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637cac68",
   "metadata": {},
   "source": [
    "Das nächst einfache Modell nach einer linearen Separation ist eines mit quadratischen Termen. Wir haben in der Vorlesung dafür die feature map\n",
    "$$\n",
    "\\phi:\\mathbb{R}^2\\to \\mathbb{R}^5,\\begin{pmatrix}\n",
    "    x_1\\\\x_2\n",
    "\\end{pmatrix}\\mapsto\\begin{pmatrix}\n",
    "    x_1\\\\x_2\\\\x_1^2\\\\x_1x_2\\\\x_2^2\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "eingeführt. \n",
    "\n",
    "__Aufgabe__: Lege die erweiterte Datenmatrix für den Trainings, Validierungs- und den Testdatensatz mit der feature map $\\phi$ an. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2792ccd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anzahl der Dimensionen des Zielraums der feature map\n",
    "d = 5\n",
    "\n",
    "# Erstellen der erweiterten Datenmatrix für den Trainingsdatensatz\n",
    "N_train = len(z_train)\n",
    "X_train = np.zeros(shape = (N_train, d+1))\n",
    "for n in range(N_train):\n",
    "    X_train[n] = \n",
    "    \n",
    "# Erstellen der ertweiterten Datenmatrix für den Validierungsdatensatz\n",
    "N_val = len(z_val)\n",
    "X_val = np.zeros(shape = (N_val, d+1))\n",
    "for n in range(N_val):\n",
    "    X_val[n] = \n",
    "    \n",
    "# Erstellen der ertweiterten Datenmatrix für den Testdatensatz\n",
    "N_test = len(z_test)\n",
    "X_test = np.zeros(shape = (N_test, d+1))\n",
    "for n in range(N_test):\n",
    "    X_test[n] = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab0d605",
   "metadata": {},
   "source": [
    "__Aufgabe__: Vervollständige das nachstehende Code-Gerüst, um einen Gradientenabstieg durchzuführen. Speichere dabei für jede Epoche die Gewichte, so wie den jeweiligen Trainings- und Validierungsfehler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fb9963",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Festlegen von Hyperparameters\n",
    "learning_rate = \n",
    "epochen = \n",
    "al = 1    # Verschärfen der Sigmoidfunktion\n",
    "\n",
    "# Initialisieren der Gewichte\n",
    "w = np.zeros(d+1)\n",
    "\n",
    "# Erstellen eines Arrays, in dem die Gewichte in jeder Epoche gespeichert werden\n",
    "W = np.zeros(shape = (epochen, d+1))\n",
    "\n",
    "# Erstelle Arrays in denen der Trainings und Validierungsfehler gespeichert werden\n",
    "R_train, R_val = np.zeros(epochen), np.zeros(epochen)\n",
    "\n",
    "# Gradientenabstieg\n",
    "for n in range(epochen):\n",
    "    # Erstellen eines leeren Gradientvektors\n",
    "    grad_vec = np.zeros(d+1)\n",
    "    \n",
    "    # Bestimmen der einzelnen Komponenten des Gradientenvektors\n",
    "    for j in range(d):\n",
    "        grad_vec[j] = \n",
    "        \n",
    "    # Updaten der Gewichte    \n",
    "    w = w-learning_rate*grad_vec\n",
    "    \n",
    "    # Speichern der Gewichte dieser Epoche\n",
    "    W[n] = w\n",
    "    \n",
    "    # Errechnen und Speichern des Trainings- und des Validierungsfehlers der Epoche\n",
    "    R_train[n] = \n",
    "    R_val[n] = \n",
    "    \n",
    "# Auftragen der Fehler \n",
    "plt.plot(R_train, 'k-')\n",
    "plt.plot(R_val, 'b--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca948c9d",
   "metadata": {},
   "source": [
    "__Aufgabe__: Bestimme durch die numpy-Funktion `np.argmin()` die Epoche mit dem geringsten Validierungsfehler. Wenn Dir die Zahl sinnvoll erscheint, setzte die Gewichte auf die Werte dieser Epoche und trage damit die Entscheidungskurve mit dem Trainings- und Validierungsdatensatz und separat mit dem Testdatensatz auf. Bestimme dann den Test-, sowie den Verallgemeinerungsfehler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a65eabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.argmin(abs(R_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90b8cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_select = \n",
    "w = W[n_select]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebaecb5",
   "metadata": {},
   "source": [
    "#### Auftragen der Ergebnisse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b523af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anlegen eines Plots\n",
    "plt.figure(figsize = (10, 5))\n",
    "\n",
    "# Auftragen der bisher betrachteten Datensätze und Ergebnisse\n",
    "plt.subplot(121)\n",
    "\n",
    "# Auftragen des Trainings und Validierungsdatensatzes \n",
    "\n",
    "\n",
    "\n",
    "# Erstellen eines Meshgrids für den Contour-Plot\n",
    "X1, X2 = np.meshgrid(np.linspace(-1.1, 1.1, 100), np.linspace(0, 1.5, 100))\n",
    "\n",
    "# Erstellen eines Contour-Plots mit erfüllen der Gleichung bei Contour == 0\n",
    "\n",
    "\n",
    "# Zurechtmachen der Achsen\n",
    "plt.axis('equal')\n",
    "\n",
    "\n",
    "# Auftragen des Testdatensatzes mit der Hypothese\n",
    "plt.subplot(122)\n",
    "\n",
    "# Auftragen des Testdatensatzes\n",
    "\n",
    "\n",
    "# Erstellen eines Contour-Plots mit erfüllen der Gleichung bei Contour == 0\n",
    "\n",
    "\n",
    "# Zurechtmachen der Achsen\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7482228c",
   "metadata": {},
   "source": [
    "#### Ermitteln des Testfehlers und des Verallgemeinerungsfehlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef49eebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bestimmen des Testfehlers\n",
    "R_test = \n",
    "\n",
    "# Bestimmen des Verallgemeinerungsfehlers\n",
    "G = \n",
    "\n",
    "print(f'Der Trainingsfehler ist durch {R_train[n_select]:.4f} gegeben.\\nDer Testfehler liegt bei {R_test:.4f} während der Verallgemeinerungsfehler {G:.4f} beträgt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
