{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1b4064b",
   "metadata": {},
   "source": [
    "# Neuronale Netze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03b4264",
   "metadata": {},
   "source": [
    "In diesem Jupyter Notebook soll das Back-Propagation Netzwerk aus der Vorleung von Grund auf implimentiert werden. Zwischen der zeidimensionalen Eingabe- und der eindimensionalen Ausgabeschicht befinden sich zwei weitere Schichten. Die erste dieser Schichten hat eine Breite von vier, während die zweite Schicht eine Breite von zwei aufweist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fb9406",
   "metadata": {},
   "source": [
    "## Laden nötiger Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22db1f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730210c6",
   "metadata": {},
   "source": [
    "## Definieren nötiger Funktionen und aufbau des Netzwerks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b73b95",
   "metadata": {},
   "source": [
    "Im Rahmen des Trainings des Neuronalen Netzes treten einige Funktion häufig auf, so dass es sinnvoll ist, diese im Vorfeld zu definieren. Dazu zählen die Sigmoid-Funktion mit Vertärker (`amplification`), deren Ableitung, die Theta-Funktion, welche für das schlussendliche Modell verwendet wird und die Ableitung der Kreuzentropie nach der Hypothese.\n",
    "\n",
    "__Aufgabe__: Implimentiere diese Funktionen im unten stehenden Code-Gerüst, so dass sie auch Vektoren entgegen nehmen können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb82de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig(x):    # Sigmoid-Funktion mit Verstärker\n",
    "    global amplification\n",
    "    return # Füge hier die Funktion ein\n",
    "\n",
    "\n",
    "def sig_prime(x):    # Ableitung der Sigmoid-Funktion\n",
    "    global amplification\n",
    "    return # Füge hier die Funktion ein\n",
    "\n",
    "\n",
    "def theta(x):    # Theta-Funktion (1, wenn Argument größer oder gleich Null; 0 sonst)\n",
    "    return # Füge hier die Funktion ein\n",
    "\n",
    "\n",
    "def dR_dh(h, y):    # Ableitung der Kreuzentropie nach der Hypothese h\n",
    "    return np.nan_to_num( , posinf = 0, neginf = 0) # Füge im ersten Argument die Funktion ein"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abef9b9",
   "metadata": {},
   "source": [
    "In jeder Schicht $l$ wird ein Vektor $\\vec{z}^{(l)}$ aus der Ausgabe der vorangegangenen Schicht $\\vec{x}^{(l-1)}$ durch die Abbildung\n",
    "$$\\vec{z}^{(l)}=\\underline{W}^{(l)}\\vec{x}^{(l-1)}+\\vec{w}^{(l)}$$\n",
    "mit einer Matrix $\\underline{W}^{(l)}$ und einem Verschiebungsvektor $\\vec{w}^{(l)}$ konstruiert. Zur Ausgabe der Schicht $l$ wird dann die Aktivierungsfunktion $\\phi^{(l)}$ auf $\\vec{z}^{(l)}$ angewandt. Hier soll $\\phi^{(l)}$ komponentenweise die Sigmoidfunktion anwenden.\n",
    "\n",
    "__Aufgabe__: Vervollständige die unten stehenden Funktionen zur Bestimmung der einzelnen $\\vec{z}^{(l)}$. Beachte dabei auch die oben beschriebene Struktur der einzelnen Schichten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bd0c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def z1(x0):    # Aktivierungsvektor der ersten Schicht\n",
    "    global w1M, w1V    # 4x2-Matrix und Verschiebungsvektor\n",
    "    return # Füge hier die Berechnung von z1 ein\n",
    "\n",
    "\n",
    "def z2(x1):    # Aktivierungsvektor der zweiten Schicht\n",
    "    global w2M, w2V    # 2x4-Matrix und Verschiebungsvektor\n",
    "    return # Füge hier die Berechnung von z2 ein\n",
    "\n",
    "\n",
    "def z3(x2):    # Aktivierungsvektor der dritten Schicht\n",
    "    global w3M, w3V    # 1x2-Matrix als Vektor und Verschiebungsvektor in Form einer Zahl\n",
    "    return # Füge hier die Berechnung von z3 ein"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ec0eb3",
   "metadata": {},
   "source": [
    "## Trainieren des Neuronalen Netzes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418e4000",
   "metadata": {},
   "source": [
    "Im Datensatz `neural_network_data01.dat` findest Du einen linear separierbaren Datensatz mit $d$ Datenpunkten.\n",
    "\n",
    "__Aufgabe__: Lade den Datensatz und trage ihn auf. Formatiere den Datensatz dann so, dass die Koordinaten in Paaren $(x_1, x_2)$ in einem Vektor `X0` der Länge $d$ vorliegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ebf15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laden des Datensatzes\n",
    "x1, x2, y = # Lade den Datensatz\n",
    "\n",
    "# Auftragen des Datensatzes\n",
    "plt.scatter()   # Trage den Datensatz mit einem Scatterplot auf\n",
    "\n",
    "plt.grid(True, color = 'grey', linestyle = '--')\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf79de83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatieren des Datensatzes\n",
    "d =    # Bestimmen die Anzahl der Datenpunkte\n",
    "X0 = np.zeros(shape = (d, 2))    # Erstellen eines leeren Arrays der passenden Form\n",
    "for n in range(d):\n",
    "    X0[n] =     # Befülle X0 mit den entsprechenden Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7646e3",
   "metadata": {},
   "source": [
    "## Initialisieren der Gewichte\n",
    "Um das Netzwerk zu trainieren, müssen zunächst die Gewichte initialisiert werden. Für die lineare Regression reicht es meist aus, alle Gewichte mit Null zu initialiseren. Für komplexere Aufgaben kann es hilfreich sein, die Gewichte radnomisiert zu initialisieren.\n",
    "\n",
    "__Aufgabe__: Vervollständige das nachstehende Code-Gerüst, um die Gewichte zu initialisieren. Achte dabei vor allem auf die oben beschriebene Struktur des neuronalen Netzes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d924c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schicht 1\n",
    "w1V = # Ertslle einen Array mit Nullen der richtigen Dimensionalität\n",
    "w1M = # Ertslle einen Array mit Nullen der richtigen Dimensionalität\n",
    "\n",
    "# Schicht 2\n",
    "w2V = # Ertslle einen Array mit Nullen der richtigen Dimensionalität\n",
    "w2M = # Ertslle einen Array mit Nullen der richtigen Dimensionalität\n",
    "\n",
    "# Schicht 3\n",
    "w3V = # Ertslle einen Array mit Nullen der richtigen Dimensionalität\n",
    "w3M = # Ertslle einen Array mit Nullen der richtigen Dimensionalität"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d63e46c",
   "metadata": {},
   "source": [
    "# Trainieren des Neuronalen Netzes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1450763a",
   "metadata": {},
   "source": [
    "Um ein Backpropagation-Netzwerk zu trainiere, werden in jedem Durchlauf mit den aktuellen Gewichten, die Daten durch das Netz gereicht und dabei alle Vektoren $\\vec{z}^{(l)}$ und $\\vec{x}^{(l)}$ gespeichert. Dies ist der _Forward Pass_. Anschließend werden von der letzten Schicht beginnend die Gradienten bzgl. der einzelnen Gewichte errechnet. Da in den Gradienten der ersten Schichten auch Ausdrück der Gradienten der späteren Schichten auftauchen, können die Gradienten der frühen Schichten so wesentlich effizient berechnet werden. Dies ist der _Backward Pass_. Schlussendlich werden mit dem Gradientenabstiegsverfahren die neuen Gewichte bestimmt. Dieser Prozess wird für mehrere Epochen wiederhohlt.\n",
    "\n",
    "In der Vorlesung haben wir gezeigt, dass die Gradienten für einen einzelnen Datenpunkt für ein Netzwerk der obigen Struktur die Form\n",
    "$$\n",
    "\\frac{\\partial R}{\\partial w^{(3)}}=\\frac{\\partial R}{\\partial h}\\mathrm{sig}'(z^{(3)})\\quad\\quad \\frac{\\partial R}{\\partial\\underline{W}^{(3)}_i}=\\frac{\\partial R}{\\partial w^{(3)}}x^{(2)}_i\\\\\n",
    "\\frac{\\partial R}{\\partial w^{(2)}_i} = \\frac{\\partial R}{\\partial w^{(3)}}\\underline{W}_i^{(3)}\\mathrm{sig}'(z_i^{(2)})\\quad\\quad \\frac{\\partial R}{\\partial \\underline{W}^{(2)}_{ij}}=\\frac{\\partial R}{\\partial w^{(2)}_i}x_j^{(1)}\\\\\n",
    "\\frac{\\partial R}{\\partial w_{i}^{(1)}} = \\left[\\left(\\nabla_{\\vec{w}^{(2)}}R\\right)^T\\underline{W}^{(2)}\\right]_i\\mathrm{sig}'(z_i^{(1)})\\quad\\quad \\frac{\\partial R}{\\partial \\underline{W}_{ij}^{(1)}}=\\frac{\\partial R}{\\partial w_i^{(1)}}x_j^{(0)}\n",
    "$$\n",
    "haben. Die tatsächlichen Gradienten ergeben sich wegen\n",
    "$$R = \\frac{1}{N}\\sum_{k = 1}^{d}\\mathrm{Kreuzentropie}(y_k, \\vec{x}_k^{(0)})$$\n",
    "dann als Summe dieser Terme.\n",
    "\n",
    "__Aufgabe__: Vervollständige das nachstehende Code-Gerüst eines Neuronalen Netzes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74036bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einstellen der Hyperparameter\n",
    "amplification = \n",
    "epochen =\n",
    "eta = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ef5bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainieren des Neuronalen Netz\n",
    "for i in range(epochen):    # Iterieren über die Epochen\n",
    "    \n",
    "    ### Forward Pass ###\n",
    "    \n",
    "    # Schicht 1\n",
    "    Z1 = np.zeros(shape = (d, 4))    # Erstellen eines leeren Arrays zum Speichern von Z1 für jeden Datenpunkt\n",
    "    for n in range(d):\n",
    "        Z1[n] =     # Ermitteln von Z1 für jeden Datenpunkt\n",
    "    X1 =     # Ermitteln von X1 für jeden Datenpunkt\n",
    "    \n",
    "    \n",
    "    # Schicht 2\n",
    "    Z2 = np.zeros(shape = (d, 2))    # Erstellen eines leeren Arrays zum Speichern von Z2 für jeden Datenpunkt\n",
    "    for n in range(d):\n",
    "        Z2[n] =     # Ermitteln von Z2 für jeden Datenpunkt\n",
    "    X2 =     # Ermitteln von X2 für jeden Datenpunkt\n",
    "    \n",
    "    \n",
    "    # Schicht 3\n",
    "    Z3 = np.zeros(d)    # Erstellen eines leeren Arrays zum Speichern von Z3 für jeden Datenpunkt\n",
    "    for n in range(d):\n",
    "        Z3[n] =    # Ermitteln von Z3 für jeden Datenpunkt\n",
    "    X3 =     # Ermitteln von X3 für jeden Datenpunkt\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### Backward Pass ###\n",
    "    \n",
    "    # Schicht 3\n",
    "    # Gradient des Verschiebungsvektors\n",
    "    dR_dw3V_temp =     # Bestimme einen Array mit den Gradienten der einzelnen Datenpunkten\n",
    "    dR_dw3V =     # Summiere die einzelnen Gradienten auf\n",
    "    \n",
    "    # Gradient der Matrix\n",
    "    dR_dw3M_temp = np.zeros(shape = (d, 2))    # Leerer Array zum Speichern des Gradienten für jeden Datenpunkt\n",
    "    for n in range(d):\n",
    "        dR_dw3M_temp[n] =    # Ermitteln des Gradienten für jeden Datenpunkt\n",
    "    dR_dw3M =    # Summieren dier einzelnen Gradienten, axis = 0 summiert nur über Datenpunkte\n",
    "\n",
    "    \n",
    "    # Schicht 2\n",
    "    # Gradient des Verschiebungsvektors\n",
    "    dR_dw2V_temp = np.zeros(shape = (d, 2))    # Leerer Array zum Speichern des Gradienten für jeden Datenpunkt\n",
    "    for n in range(d):\n",
    "        dR_dw2V_temp[n] =     # Ermitteln des Gradienten für jeden Datenpunkt\n",
    "    dR_dw2V =     # Summieren dier einzelnen Gradienten, axis = 0 summiert nur über Datenpunkte\n",
    "\n",
    "    # Gradient der Matrix\n",
    "    dR_dw2M_temp = np.zeros(shape = (d, 2, 4))    # Leerer Array zum Speichern des Gradienten für jeden Datenpunkt\n",
    "    for n in range(d):\n",
    "        dR_dw2M_temp[n] = np.outer()    # Ermitteln des Gradienten für jeden Datenpunkt; benutze hierzu np.outer\n",
    "    dR_dw2M =     # Summieren dier einzelnen Gradienten, axis = 0 summiert nur über Datenpunkte\n",
    "\n",
    "    \n",
    "    # Schicht 1\n",
    "    # Gradient des Verschiebungsvektors\n",
    "    dR_dw1V_temp = np.zeros(shape = (d, 4))    # Leerer Array zum Speichern des Gradienten für jeden Datenpunkt\n",
    "    for n in range(d):\n",
    "        dR_dw1V_temp[n] =     # Ermitteln des Gradienten für jeden Datenpunkt\n",
    "    dR_dw1V =     # Summieren dier einzelnen Gradienten, axis = 0 summiert nur über Datenpunkte\n",
    "\n",
    "    # Gradient der Matrix\n",
    "    dR_dw1M_temp = np.zeros(shape = (d, 4, 2))    # Leerer Array zum Speichern des Gradienten für jeden Datenpunkt\n",
    "    for n in range(d):\n",
    "        dR_dw1M_temp[n] = np.outer()    # Ermitteln des Gradienten für jeden Datenpunkt; Benutze hierzu np.outer\n",
    "\n",
    "    dR_dw1M =     # Summieren dier einzelnen Gradienten, axis = 0 summiert nur über Datenpunkte\n",
    "    \n",
    "    \n",
    "    ### Updaten der Gewichte ###\n",
    "    \n",
    "    # Schicht 1\n",
    "    w1M = \n",
    "    w1V = \n",
    "    \n",
    "    # Schicht 2\n",
    "    w2M = \n",
    "    w2V = \n",
    "    \n",
    "    # Schicht 3\n",
    "    w3M = \n",
    "    w3V = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050417e8",
   "metadata": {},
   "source": [
    "## Auswerten eines Datenpunktes\n",
    "Um das Ergebnis des trainierten neuronalen Netzes bestimmen zu können, muss eine Funktion mit der Hypothese mit der Theta-Funktion in letzter Instanz definiert werden.\n",
    "\n",
    "__Aufgabe__: Vervollständige die nachstehende Funktion, um die Klassifikation durch das Netz durch ihren Aufruf bestimmen zu können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed85f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(x):\n",
    "    return # Füge hier die Funktion zum Auswerten des Neuronalen Netzes ein"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3817ce38",
   "metadata": {},
   "source": [
    "## Auftragen der Ergebnisse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b739b67",
   "metadata": {},
   "source": [
    "In der ersten nachfolgenden Zelle ist ganz links der vorliegende Datensatz zu sehen. Nach rechts fortschreitend sind dann die Aktivierungen der einzelnen Neuronen aufgetragen. Ganz rechts ist somit das Ergebnis des Neuronalen Netzes für die vorliegenden Datenpunkte zu sehen.\n",
    "\n",
    "In der zweiten Zelle wird ein Contour-Plot mit den Ergebnissen des Neuronalen Netzes erstelln. Die vorliegenden Daten werden farblich markiert eingezeichnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff890ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auftragen der Aktivierung in den einzelnen Schichten\n",
    "plt.figure(figsize = (20, 20))\n",
    "\n",
    "# Subplot für Eingabeschicht\n",
    "plt.subplot(441)\n",
    "\n",
    "plt.scatter(x1, x2, c = y, cmap = 'seismic')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, color = 'grey', linestyle = '--')\n",
    "\n",
    "\n",
    "# Subplot für Schicht 1, Neuron 1\n",
    "plt.subplot(442)\n",
    "\n",
    "h_eval = np.zeros(d)\n",
    "for n in range(d):\n",
    "    h_eval[n] = sig(z1(X0[n]))[0]\n",
    "\n",
    "plt.scatter(x1, x2, c = h_eval, cmap = 'seismic')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, color = 'grey', linestyle = '--')\n",
    "\n",
    "# Subplot für Schicht 1, Neuron 2\n",
    "plt.subplot(446)\n",
    "\n",
    "h_eval = np.zeros(d)\n",
    "for n in range(d):\n",
    "    h_eval[n] = sig(z1(X0[n]))[1]\n",
    "\n",
    "plt.scatter(x1, x2, c = h_eval, cmap = 'seismic')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, color = 'grey', linestyle = '--')\n",
    "\n",
    "# Subplot für Schicht 1, Neuron 3\n",
    "plt.subplot(4, 4, 10)\n",
    "\n",
    "h_eval = np.zeros(d)\n",
    "for n in range(d):\n",
    "    h_eval[n] = sig(z1(X0[n]))[2]\n",
    "\n",
    "plt.scatter(x1, x2, c = h_eval, cmap = 'seismic')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, color = 'grey', linestyle = '--')\n",
    "\n",
    "# Subplot für Schicht 1, Neuron 4\n",
    "plt.subplot(4, 4, 14)\n",
    "\n",
    "h_eval = np.zeros(d)\n",
    "for n in range(d):\n",
    "    h_eval[n] = sig(z1(X0[n]))[0]\n",
    "\n",
    "plt.scatter(x1, x2, c = h_eval, cmap = 'seismic')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, color = 'grey', linestyle = '--')\n",
    "\n",
    "# Subplot für Schicht 3, Neuron 1\n",
    "plt.subplot(443)\n",
    "\n",
    "h_eval = np.zeros(d)\n",
    "for n in range(d):\n",
    "    h_eval[n] = sig(z2(sig(z1(X0[n]))))[0]\n",
    "\n",
    "plt.scatter(x1, x2, c = h_eval, cmap = 'seismic')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, color = 'grey', linestyle = '--')\n",
    "\n",
    "# Subplot für Schicht 3, Neuron 2\n",
    "plt.subplot(447)\n",
    "\n",
    "h_eval = np.zeros(d)\n",
    "for n in range(d):\n",
    "    h_eval[n] = sig(z2(sig(z1(X0[n]))))[1]\n",
    "\n",
    "plt.scatter(x1, x2, c = h_eval, cmap = 'seismic')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, color = 'grey', linestyle = '--')\n",
    "\n",
    "# Subplot für Ausgabeschicht\n",
    "plt.subplot(444)\n",
    "\n",
    "h_eval = np.zeros(d)\n",
    "for n in range(d):\n",
    "    h_eval[n] = h(X0[n])\n",
    "\n",
    "plt.scatter(x1, x2, c = h_eval, cmap = 'seismic')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, color = 'grey', linestyle = '--')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f3da06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auftragung der Klassifikation des Netzes\n",
    "plt.figure(figsize = (5, 5))\n",
    "\n",
    "# Anlegen eines meshgrids\n",
    "N = 100\n",
    "X, Y = np.meshgrid(np.linspace(0, 1, N), np.linspace(0, 1, N))\n",
    "\n",
    "h_eval = np.zeros(shape = (N, N))    # Leerer Array zum Speiechern er Klassifikation durch das Netz\n",
    "    \n",
    "for n in range(N):\n",
    "    for m in range(N):\n",
    "        h_eval[n][m] = h(np.array([X[n][m], Y[n][m]]))    # Ermitteln der Klassifikation durch das Netz\n",
    "        \n",
    "# Auftragen der Klassifikation durch das Netz\n",
    "plt.contourf(X, Y, h_eval, cmap = 'seismic')\n",
    "\n",
    "# Auftragen der Datenpunkte\n",
    "plt.scatter(x1, x2, c = y , cmap = 'spring')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a568b6",
   "metadata": {},
   "source": [
    "## Nicht linear separierbare Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546ee60e",
   "metadata": {},
   "source": [
    "Neuronale Netze mit mehreren Schichten (Deep Learning) sind in der Lage auch nicht linear separierbare Daten ohne händisches Feature Engineering klassifizieren zu können. Dazu sind im Datensatz `neural_network_data02.dat` nicht linear separierbare Daten zu finden.\n",
    "\n",
    "__Aufgabe__: Trage den Datensatz auf und formatiere diesen, um ihn mit dem Neuronalen Netz klassifizieren zu können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b4f1ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdc6bf05",
   "metadata": {},
   "source": [
    "## Trainieren eines Neuronal Netzes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176a5022",
   "metadata": {},
   "source": [
    "Mit der gleichen Netzarchitektur wie oben beschrieben, kann auch diese Datensatz klassifiziert werden. Dazu bietet es sich jedoch an, die Gewichte mit zufälligen Werten aus dem Intervall $[-1, 1]$ zu initialiseren. \n",
    "\n",
    "__Aufgabe__: Trainiere ein Neuronales Netz, dass diesen Datensatz klassifziert. Prüfe mit den Auftragungen der Aktivierung und der Klassifikation, ob eine passende Klassifikation erfolgt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee481630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4f5dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Auftragen der Aktivierung in den einzelnen Schichten ###\n",
    "plt.figure(figsize = (20, 20))\n",
    "\n",
    "# Subplot für Eingabeschicht\n",
    "plt.subplot(441)\n",
    "\n",
    "plt.scatter(x1, x2, c = y, cmap = 'seismic')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, color = 'grey', linestyle = '--')\n",
    "\n",
    "\n",
    "# Subplot für Schicht 1, Neuron 1\n",
    "plt.subplot(442)\n",
    "\n",
    "h_eval = np.zeros(d)\n",
    "for n in range(d):\n",
    "    h_eval[n] = sig(z1(X0[n]))[0]\n",
    "\n",
    "plt.scatter(x1, x2, c = h_eval, cmap = 'seismic')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, color = 'grey', linestyle = '--')\n",
    "\n",
    "# Subplot für Schicht 1, Neuron 2\n",
    "plt.subplot(446)\n",
    "\n",
    "h_eval = np.zeros(d)\n",
    "for n in range(d):\n",
    "    h_eval[n] = sig(z1(X0[n]))[1]\n",
    "\n",
    "plt.scatter(x1, x2, c = h_eval, cmap = 'seismic')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, color = 'grey', linestyle = '--')\n",
    "\n",
    "# Subplot für Schicht 1, Neuron 3\n",
    "plt.subplot(4, 4, 10)\n",
    "\n",
    "h_eval = np.zeros(d)\n",
    "for n in range(d):\n",
    "    h_eval[n] = sig(z1(X0[n]))[2]\n",
    "\n",
    "plt.scatter(x1, x2, c = h_eval, cmap = 'seismic')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, color = 'grey', linestyle = '--')\n",
    "\n",
    "# Subplot für Schicht 1, Neuron 4\n",
    "plt.subplot(4, 4, 14)\n",
    "\n",
    "h_eval = np.zeros(d)\n",
    "for n in range(d):\n",
    "    h_eval[n] = sig(z1(X0[n]))[0]\n",
    "\n",
    "plt.scatter(x1, x2, c = h_eval, cmap = 'seismic')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, color = 'grey', linestyle = '--')\n",
    "\n",
    "# Subplot für Schicht 3, Neuron 1\n",
    "plt.subplot(443)\n",
    "\n",
    "h_eval = np.zeros(d)\n",
    "for n in range(d):\n",
    "    h_eval[n] = sig(z2(sig(z1(X0[n]))))[0]\n",
    "\n",
    "plt.scatter(x1, x2, c = h_eval, cmap = 'seismic')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, color = 'grey', linestyle = '--')\n",
    "\n",
    "# Subplot für Schicht 3, Neuron 2\n",
    "plt.subplot(447)\n",
    "\n",
    "h_eval = np.zeros(d)\n",
    "for n in range(d):\n",
    "    h_eval[n] = sig(z2(sig(z1(X0[n]))))[1]\n",
    "\n",
    "plt.scatter(x1, x2, c = h_eval, cmap = 'seismic')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, color = 'grey', linestyle = '--')\n",
    "\n",
    "# Subplot für Ausgabeschicht\n",
    "plt.subplot(444)\n",
    "\n",
    "h_eval = np.zeros(d)\n",
    "for n in range(d):\n",
    "    h_eval[n] = h(X0[n])\n",
    "\n",
    "plt.scatter(x1, x2, c = h_eval, cmap = 'seismic')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, color = 'grey', linestyle = '--')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b60238",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Auftragung der Klassifikation des Netzes ###\n",
    "plt.figure(figsize = (5, 5))\n",
    "\n",
    "# Anlegen eines meshgrids\n",
    "N = 100\n",
    "X, Y = np.meshgrid(np.linspace(-1, 1, N), np.linspace(-1, 1, N))\n",
    "\n",
    "h_eval = np.zeros(shape = (N, N))    # Leerer Array zum Speiechern er Klassifikation durch das Netz\n",
    "    \n",
    "for n in range(N):\n",
    "    for m in range(N):\n",
    "        h_eval[n][m] = h(np.array([X[n][m], Y[n][m]]))    # Ermitteln der Klassifikation durch das Netz\n",
    "        \n",
    "# Auftragen der Klassifikation durch das Netz\n",
    "plt.contourf(X, Y, h_eval, cmap = 'seismic')\n",
    "\n",
    "# Auftragen der Datenpunkte\n",
    "plt.scatter(x1, x2, c = y , cmap = 'spring')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
