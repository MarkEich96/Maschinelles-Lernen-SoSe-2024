{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d282792",
   "metadata": {},
   "source": [
    "# Neuronale Netze mit Lösungen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8fc6e4",
   "metadata": {},
   "source": [
    "In diesem Jupyter Notebook soll das Back-Propagation Netzwerk aus der Vorleung von Grund auf implimentiert werden. Zwischen der zeidimensionalen Eingabe- und der eindimensionalen Ausgabeschicht befinden sich zwei weitere Schichten. Die erste dieser Schichten hat eine Breite von vier, während die zweite Schicht eine Breite von zwei aufweist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d4059c",
   "metadata": {},
   "source": [
    "## Laden nötiger Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5d6478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cea7752",
   "metadata": {},
   "source": [
    "## Definieren nötiger Funktionen und aufbau des Netzwerks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be54fd2",
   "metadata": {},
   "source": [
    "Im Rahmen des Trainings des Neuronalen Netzes treten einige Funktion häufig auf, so dass es sinnvoll ist, diese im Vorfeld zu definieren. Dazu zählen die Sigmoid-Funktion mit Vertärker (`amplification`), deren Ableitung, die Theta-Funktion, welche für das schlussendliche Modell verwendet wird und die Ableitung der Kreuzentropie nach der Hypothese.\n",
    "\n",
    "__Aufgabe__: Implimentiere diese Funktionen im unten stehenden Code-Gerüst, so dass sie auch Vektoren entgegen nehmen können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4a5ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig(x):    # Sigmoid-Funktion mit Verstärker\n",
    "    global amplification\n",
    "    return 1/(1+np.exp(-amplification*x))\n",
    "\n",
    "\n",
    "def sig_prime(x):    # Ableitung der Sigmoid-Funktion\n",
    "    global amplification\n",
    "    return sig(x)*(1-sig(x))*amplification\n",
    "\n",
    "\n",
    "def theta(x):    # Theta-Funktion (1, wenn Argument größer oder gleich Null; 0 sonst)\n",
    "    return x >= 0\n",
    "\n",
    "\n",
    "def dR_dh(h, y):    # Ableitung der Kreuzentropie nach der Hypothese h\n",
    "    return np.nan_to_num((h-y)/(h*(1-h)), posinf = 0, neginf = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf77f2df",
   "metadata": {},
   "source": [
    "In jeder Schicht $l$ wird ein Vektor $\\vec{z}^{(l)}$ aus der Ausgabe der vorangegangenen Schicht $\\vec{x}^{(l-1)}$ durch die Abbildung\n",
    "$$\\vec{z}^{(l)}=\\underline{W}^{(l)}\\vec{x}^{(l-1)}+\\vec{w}^{(l)}$$\n",
    "mit einer Matrix $\\underline{W}^{(l)}$ und einem Verschiebungsvektor $\\vec{w}^{(l)}$ konstruiert. Zur Ausgabe der Schicht $l$ wird dann die Aktivierungsfunktion $\\phi^{(l)}$ auf $\\vec{z}^{(l)}$ angewandt. Hier soll $\\phi^{(l)}$ komponentenweise die Sigmoidfunktion anwenden.\n",
    "\n",
    "__Aufgabe__: Vervollständige die unten stehenden Funktionen zur Bestimmung der einzelnen $\\vec{z}^{(l)}$. Beachte dabei auch die oben beschriebene Struktur der einzelnen Schichten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edae17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def z1(x0):    # Aktivierungsvektor der ersten Schicht\n",
    "    global w1M, w1V    # 4x2-Matrix und Verschiebungsvektor\n",
    "    return w1M@x0+w1V\n",
    "\n",
    "\n",
    "def z2(x1):    # Aktivierungsvektor der zweiten Schicht\n",
    "    global w2M, w2V    # 2x4-Matrix und Verschiebungsvektor\n",
    "    return w2M@x1+w2V\n",
    "\n",
    "\n",
    "def z3(x2):    # Aktivierungsvektor der dritten Schicht\n",
    "    global w3M, w3V    # 1x2-Matrix als Vektor und Verschiebungsvektor in Form einer Zahl\n",
    "    return w3M.T@x2+w3V    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4793e7",
   "metadata": {},
   "source": [
    "## Trainieren des Neuronalen Netzes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52420e8d",
   "metadata": {},
   "source": [
    "Im Datensatz `neural_network_data01.dat` findest Du einen linear separierbaren Datensatz mit $d$ Datenpunkten.\n",
    "\n",
    "__Aufgabe__: Lade den Datensatz und trage ihn auf. Formatiere den Datensatz dann so, dass die Koordinaten in Paaren $(x_1, x_2)$ in einem Vektor `X0` der Länge $d$ vorliegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811bdeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laden des Datensatzes\n",
    "x1, x2, y = np.loadtxt('neural_network_data01.dat', unpack = True)\n",
    "\n",
    "# Auftragen des Datensatzes\n",
    "plt.scatter(x1, x2, c = y, cmap = 'seismic')\n",
    "\n",
    "plt.grid(True, color = 'grey', linestyle = '--')\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43930e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatieren des Datensatzes\n",
    "d = len(y)    # Bestimmen der Anzahl der Datenpunkte\n",
    "X0 = np.zeros(shape = (d, 2))    # Erstellen eines leeren Arrays der passenden Form\n",
    "for n in range(d):\n",
    "    X0[n] = np.array([x1[n], x2[n]])    # Auffüllen des Arrays mit den entsprechenden Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b0ec22",
   "metadata": {},
   "source": [
    "## Initialisieren der Gewichte\n",
    "Um das Netzwerk zu trainieren, müssen zunächst die Gewichte initialisiert werden. Für die lineare Regression reicht es meist aus, alle Gewichte mit Null zu initialiseren. Für komplexere Aufgaben kann es hilfreich sein, die Gewichte radnomisiert zu initialisieren.\n",
    "\n",
    "__Aufgabe__: Vervollständige das nachstehende Code-Gerüst, um die Gewichte zu initialisieren. Achte dabei vor allem auf die oben beschriebene Struktur des neuronalen Netzes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132cca48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schicht 1\n",
    "w1V = np.zeros(4)    # Verschiebungsvektor der Dimension 4\n",
    "w1M = np.zeros(shape = (4, 2))   # 4x2-Matrix\n",
    "\n",
    "# Schicht 2\n",
    "w2V = np.zeros(2)   # Verschiebungsvektor der Dimension 2\n",
    "w2M = np.zeros(shape = (2, 4))    # 2x4-Matrix\n",
    "\n",
    "# Schicht 3\n",
    "w3V = 0    # Verschiebungsvektor der Dimension 1 -> Zahl\n",
    "w3M = np.zeros(2)    # 1x2-Matrix, hier dargestellt als Vektor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175f9e4d",
   "metadata": {},
   "source": [
    "# Trainieren des Neuronalen Netzes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db88d0f3",
   "metadata": {},
   "source": [
    "Um ein Backpropagation-Netzwerk zu trainiere, werden in jedem Durchlauf mit den aktuellen Gewichten, die Daten durch das Netz gereicht und dabei alle Vektoren $\\vec{z}^{(l)}$ und $\\vec{x}^{(l)}$ gespeichert. Dies ist der _Forward Pass_. Anschließend werden von der letzten Schicht beginnend die Gradienten bzgl. der einzelnen Gewichte errechnet. Da in den Gradienten der ersten Schichten auch Ausdrück der Gradienten der späteren Schichten auftauchen, können die Gradienten der frühen Schichten so wesentlich effizient berechnet werden. Dies ist der _Backward Pass_. Schlussendlich werden mit dem Gradientenabstiegsverfahren die neuen Gewichte bestimmt. Dieser Prozess wird für mehrere Epochen wiederhohlt.\n",
    "\n",
    "In der Vorlesung haben wir gezeigt, dass die Gradienten für einen einzelnen Datenpunkt für ein Netzwerk der obigen Struktur die Form\n",
    "$$\n",
    "\\frac{\\partial R}{\\partial w^{(3)}}=\\frac{\\partial R}{\\partial h}\\mathrm{sig}'(z^{(3)})\\quad\\quad \\frac{\\partial R}{\\partial\\underline{W}^{(3)}_i}=\\frac{\\partial R}{\\partial w^{(3)}}x^{(2)}_i\\\\\n",
    "\\frac{\\partial R}{\\partial w^{(2)}_i} = \\frac{\\partial R}{\\partial w^{(3)}}\\underline{W}_i^{(3)}\\mathrm{sig}'(z_i^{(2)})\\quad\\quad \\frac{\\partial R}{\\partial \\underline{W}^{(2)}_{ij}}=\\frac{\\partial R}{\\partial w^{(2)}_i}x_j^{(1)}\\\\\n",
    "\\frac{\\partial R}{\\partial w_{i}^{(1)}} = \\left[\\left(\\nabla_{\\vec{w}^{(2)}}R\\right)^T\\underline{W}^{(2)}\\right]_i\\mathrm{sig}'(z_i^{(1)})\\quad\\quad \\frac{\\partial R}{\\partial \\underline{W}_{ij}^{(1)}}=\\frac{\\partial R}{\\partial w_i^{(1)}}x_j^{(0)}\n",
    "$$\n",
    "haben. Die tatsächlichen Gradienten ergeben sich wegen\n",
    "$$R = \\frac{1}{N}\\sum_{k = 1}^{d}\\mathrm{Kreuzentropie}(y_k, \\vec{x}_k^{(0)})$$\n",
    "dann als Summe dieser Terme.\n",
    "\n",
    "__Aufgabe__: Vervollständige das nachstehende Code-Gerüst eines Neuronalen Netzes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7a7e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einstellen der Hyperparameter\n",
    "amplification = 10\n",
    "epochen = 1000\n",
    "eta = 10**(-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f79ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainieren des Neuronalen Netz\n",
    "for i in range(epochen):    # Iterieren über die Epochen\n",
    "    \n",
    "    ### Forward Pass ###\n",
    "    \n",
    "    # Schicht 1\n",
    "    Z1 = np.zeros(shape = (d, 4))    # Erstellen eines leeren Arrays zum Speichern von Z1 für jeden Datenpunkt\n",
    "    for n in range(d):\n",
    "        Z1[n] = z1(X0[n])    # Ermitteln von Z1 für jeden Datenpunkt\n",
    "    X1 = sig(Z1)    # Ermitteln von X1 für jeden Datenpunkt\n",
    "    \n",
    "    \n",
    "    # Schicht 2\n",
    "    Z2 = np.zeros(shape = (d, 2))    # Erstellen eines leeren Arrays zum Speichern von Z2 für jeden Datenpunkt\n",
    "    for n in range(d):\n",
    "        Z2[n] = z2(X1[n])    # Ermitteln von Z2 für jeden Datenpunkt\n",
    "    X2 = sig(Z2)    # Ermitteln von X2 für jeden Datenpunkt\n",
    "    \n",
    "    \n",
    "    # Schicht 3\n",
    "    Z3 = np.zeros(d)    # Erstellen eines leeren Arrays zum Speichern von Z3 für jeden Datenpunkt\n",
    "    for n in range(d):\n",
    "        Z3[n] = z3(X2[n])    # Ermitteln von Z3 für jeden Datenpunkt\n",
    "    X3 = sig(Z3)    # Ermitteln von X3 für jeden Datenpunkt\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### Backward Pass ###\n",
    "    \n",
    "    # Schicht 3\n",
    "    # Gradient des Verschiebungsvektors\n",
    "    dR_dw3V_temp = dR_dh(X3, y)*sig_prime(Z3)    # Bestimme einen Array mit den Gradienten der einzelnen Datenpunkten\n",
    "    dR_dw3V = np.sum(dR_dw3V_temp)    # Summiere die einzelnen Gradienten auf\n",
    "    \n",
    "    # Gradient der Matrix\n",
    "    dR_dw3M_temp = np.zeros(shape = (d, 2))    # Leerer Array zum Speichern des Gradienten für jeden Datenpunkt\n",
    "    for n in range(d):\n",
    "        dR_dw3M_temp[n] = dR_dw3V_temp[n]*X2[n]    # Ermitteln des Gradienten für jeden Datenpunkt\n",
    "    dR_dw3M = np.sum(dR_dw3M_temp, axis = 0)    # Summieren dier einzelnen Gradienten, axis = 0 summiert nur über Datenpunkte\n",
    "\n",
    "    \n",
    "    # Schicht 2\n",
    "    # Gradient des Verschiebungsvektors\n",
    "    dR_dw2V_temp = np.zeros(shape = (d, 2))    # Leerer Array zum Speichern des Gradienten für jeden Datenpunkt\n",
    "    for n in range(d):\n",
    "        dR_dw2V_temp[n] = dR_dw3V_temp[n]*w3M*sig_prime(Z2[n])    # Ermitteln des Gradienten für jeden Datenpunkt\n",
    "    dR_dw2V = np.sum(dR_dw2V_temp, axis = 0)    # Summieren dier einzelnen Gradienten, axis = 0 summiert nur über Datenpunkte\n",
    "\n",
    "    # Gradient der Matrix\n",
    "    dR_dw2M_temp = np.zeros(shape = (d, 2, 4))    # Leerer Array zum Speichern des Gradienten für jeden Datenpunkt\n",
    "    for n in range(d):\n",
    "        dR_dw2M_temp[n] = np.outer(dR_dw2V_temp[n], X1[n])    # Ermitteln des Gradienten für jeden Datenpunkt\n",
    "    dR_dw2M = np.sum(dR_dw2M_temp, axis = 0)    # Summieren dier einzelnen Gradienten, axis = 0 summiert nur über Datenpunkte\n",
    "\n",
    "    \n",
    "    # Schicht 1\n",
    "    # Gradient des Verschiebungsvektors\n",
    "    dR_dw1V_temp = np.zeros(shape = (d, 4))    # Leerer Array zum Speichern des Gradienten für jeden Datenpunkt\n",
    "    for n in range(d):\n",
    "        dR_dw1V_temp[n] = (dR_dw2V_temp[n].T@w2M)*sig_prime(Z1[n])    # Ermitteln des Gradienten für jeden Datenpunkt\n",
    "    dR_dw1V = np.sum(dR_dw1V_temp, axis = 0)    # Summieren dier einzelnen Gradienten, axis = 0 summiert nur über Datenpunkte\n",
    "\n",
    "    # Gradient der Matrix\n",
    "    dR_dw1M_temp = np.zeros(shape = (d, 4, 2))    # Leerer Array zum Speichern des Gradienten für jeden Datenpunkt\n",
    "    for n in range(d):\n",
    "        dR_dw1M_temp[n] = np.outer(dR_dw1V_temp[n], X0[n])    # Ermitteln des Gradienten für jeden Datenpunkt\n",
    "\n",
    "    dR_dw1M = np.sum(dR_dw1M_temp, axis = 0)    # Summieren dier einzelnen Gradienten, axis = 0 summiert nur über Datenpunkte\n",
    "    \n",
    "    \n",
    "    ### Updaten der Gewichte ###\n",
    "    \n",
    "    # Schicht 1\n",
    "    w1M = w1M-eta*dR_dw1M\n",
    "    w1V = w1V-eta*dR_dw1V\n",
    "    \n",
    "    # Schicht 2\n",
    "    w2M = w2M-eta*dR_dw2M\n",
    "    w2V = w2V-eta*dR_dw2V\n",
    "    \n",
    "    # Schicht 3\n",
    "    w3M = w3M-eta*dR_dw3M\n",
    "    w3V = w3V-eta*dR_dw3V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63865e66",
   "metadata": {},
   "source": [
    "## Auswerten eines Datenpunktes\n",
    "Um das Ergebnis des trainierten neuronalen Netzes bestimmen zu können, muss eine Funktion mit der Hypothese mit der Theta-Funktion in letzter Instanz definiert werden.\n",
    "\n",
    "__Aufgabe__: Vervollständige die nachstehende Funktion, um die Klassifikation durch das Netz durch ihren Aufruf bestimmen zu können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b067ea95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(x):\n",
    "    return theta(z3(sig(z2(sig(z1(x))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830123a7",
   "metadata": {},
   "source": [
    "## Auftragen der Ergebnisse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66914c83",
   "metadata": {},
   "source": [
    "In der ersten nachfolgenden Zelle ist ganz links der vorliegende Datensatz zu sehen. Nach rechts fortschreitend sind dann die Aktivierungen der einzelnen Neuronen aufgetragen. Ganz rechts ist somit das Ergebnis des Neuronalen Netzes für die vorliegenden Datenpunkte zu sehen.\n",
    "\n",
    "In der zweiten Zelle wird ein Contour-Plot mit den Ergebnissen des Neuronalen Netzes erstelln. Die vorliegenden Daten werden farblich markiert eingezeichnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46894ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auftragen der Aktivierung in den einzelnen Schichten\n",
    "plt.figure(figsize = (20, 20))\n",
    "\n",
    "# Subplot für Eingabeschicht\n",
    "plt.subplot(441)\n",
    "\n",
    "plt.scatter(x1, x2, c = y, cmap = 'seismic')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, color = 'grey', linestyle = '--')\n",
    "\n",
    "\n",
    "# Subplot für Schicht 1, Neuron 1\n",
    "plt.subplot(442)\n",
    "\n",
    "h_eval = np.zeros(d)\n",
    "for n in range(d):\n",
    "    h_eval[n] = sig(z1(X0[n]))[0]\n",
    "\n",
    "plt.scatter(x1, x2, c = h_eval, cmap = 'seismic')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, color = 'grey', linestyle = '--')\n",
    "\n",
    "# Subplot für Schicht 1, Neuron 2\n",
    "plt.subplot(446)\n",
    "\n",
    "h_eval = np.zeros(d)\n",
    "for n in range(d):\n",
    "    h_eval[n] = sig(z1(X0[n]))[1]\n",
    "\n",
    "plt.scatter(x1, x2, c = h_eval, cmap = 'seismic')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, color = 'grey', linestyle = '--')\n",
    "\n",
    "# Subplot für Schicht 1, Neuron 3\n",
    "plt.subplot(4, 4, 10)\n",
    "\n",
    "h_eval = np.zeros(d)\n",
    "for n in range(d):\n",
    "    h_eval[n] = sig(z1(X0[n]))[2]\n",
    "\n",
    "plt.scatter(x1, x2, c = h_eval, cmap = 'seismic')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, color = 'grey', linestyle = '--')\n",
    "\n",
    "# Subplot für Schicht 1, Neuron 4\n",
    "plt.subplot(4, 4, 14)\n",
    "\n",
    "h_eval = np.zeros(d)\n",
    "for n in range(d):\n",
    "    h_eval[n] = sig(z1(X0[n]))[0]\n",
    "\n",
    "plt.scatter(x1, x2, c = h_eval, cmap = 'seismic')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, color = 'grey', linestyle = '--')\n",
    "\n",
    "# Subplot für Schicht 3, Neuron 1\n",
    "plt.subplot(443)\n",
    "\n",
    "h_eval = np.zeros(d)\n",
    "for n in range(d):\n",
    "    h_eval[n] = sig(z2(sig(z1(X0[n]))))[0]\n",
    "\n",
    "plt.scatter(x1, x2, c = h_eval, cmap = 'seismic')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, color = 'grey', linestyle = '--')\n",
    "\n",
    "# Subplot für Schicht 3, Neuron 2\n",
    "plt.subplot(447)\n",
    "\n",
    "h_eval = np.zeros(d)\n",
    "for n in range(d):\n",
    "    h_eval[n] = sig(z2(sig(z1(X0[n]))))[1]\n",
    "\n",
    "plt.scatter(x1, x2, c = h_eval, cmap = 'seismic')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, color = 'grey', linestyle = '--')\n",
    "\n",
    "# Subplot für Ausgabeschicht\n",
    "plt.subplot(444)\n",
    "\n",
    "h_eval = np.zeros(d)\n",
    "for n in range(d):\n",
    "    h_eval[n] = h(X0[n])\n",
    "\n",
    "plt.scatter(x1, x2, c = h_eval, cmap = 'seismic')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, color = 'grey', linestyle = '--')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d7e724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auftragung der Klassifikation des Netzes\n",
    "plt.figure(figsize = (5, 5))\n",
    "\n",
    "# Anlegen eines meshgrids\n",
    "N = 100\n",
    "X, Y = np.meshgrid(np.linspace(0, 1, N), np.linspace(0, 1, N))\n",
    "\n",
    "h_eval = np.zeros(shape = (N, N))    # Leerer Array zum Speiechern er Klassifikation durch das Netz\n",
    "    \n",
    "for n in range(N):\n",
    "    for m in range(N):\n",
    "        h_eval[n][m] = h(np.array([X[n][m], Y[n][m]]))    # Ermitteln der Klassifikation durch das Netz\n",
    "        \n",
    "# Auftragen der Klassifikation durch das Netz\n",
    "plt.contourf(X, Y, h_eval, cmap = 'seismic')\n",
    "\n",
    "# Auftragen der Datenpunkte\n",
    "plt.scatter(x1, x2, c = y , cmap = 'spring')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4698d86b",
   "metadata": {},
   "source": [
    "## Nicht linear separierbare Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2261f9",
   "metadata": {},
   "source": [
    "Neuronale Netze mit mehreren Schichten (Deep Learning) sind in der Lage auch nicht linear separierbare Daten ohne händisches Feature Engineering klassifizieren zu können. Dazu sind im Datensatz `neural_network_data02.dat` nicht linear separierbare Daten zu finden.\n",
    "\n",
    "__Aufgabe__: Trage den Datensatz auf und formatiere diesen, um ihn mit dem Neuronalen Netz klassifizieren zu können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82febd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laden und Auftragen der Daten\n",
    "x1, x2, y = np.loadtxt(\"neural_network_data02.dat\", unpack = True)\n",
    "\n",
    "# Formatieren des Datensatzes\n",
    "d = len(y)    # Bestimmen der Anzahl der Datenpunkte\n",
    "X0 = np.zeros(shape = (d, 2))    # Erstellen eines leeren Arrays der passenden Form\n",
    "for n in range(d):\n",
    "    X0[n] = np.array([x1[n], x2[n]])    # Auffüllen des Arrays mit den entsprechenden Daten\n",
    "\n",
    "# Auftragen des Datensatzes\n",
    "plt.scatter(x1, x2, c = y, cmap = 'seismic')\n",
    "\n",
    "plt.grid(True, color = 'grey', linestyle = '--')\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14458da2",
   "metadata": {},
   "source": [
    "## Trainieren eines Neuronal Netzes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef87296e",
   "metadata": {},
   "source": [
    "Mit der gleichen Netzarchitektur wie oben beschrieben, kann auch diese Datensatz klassifiziert werden. Dazu bietet es sich jedoch an, die Gewichte mit zufälligen Werten aus dem Intervall $[-1, 1]$ zu initialiseren. \n",
    "\n",
    "__Aufgabe__: Trainiere ein Neuronales Netz, dass diesen Datensatz klassifziert. Prüfe mit den Auftragungen der Aktivierung und der Klassifikation, ob eine passende Klassifikation erfolgt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fb1afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialisieren der Gewichte ###\n",
    "\n",
    "np.random.seed(15646)\n",
    "\n",
    "# Schicht 1\n",
    "w1V = np.random.rand(4)*2-1    # Verschiebungsvektor der Dimension 4\n",
    "w1M = np.random.rand(4, 2)*2-1   # 4x2-Matrix\n",
    "\n",
    "# Schicht 2\n",
    "w2V = np.random.rand(2)*2-1   # Verschiebungsvektor der Dimension 2\n",
    "w2M = np.random.rand(2, 4)*2-1    # 2x4-Matrix\n",
    "\n",
    "# Schicht 3\n",
    "w3V = np.random.rand()*2-1    # Verschiebungsvektor der Dimension 1 -> Zahl\n",
    "w3M = np.random.rand(2)*2-1    # 1x2-Matrix, hier dargestellt als Vektor\n",
    "\n",
    "### Einstellen der Hyperparameter ###\n",
    "amplification = 5\n",
    "epochen = 15000\n",
    "eta = 10**(-3)\n",
    "\n",
    "### Trainieren des Netzes ###\n",
    "for i in range(epochen):    # Iterieren über die Epochen\n",
    "    \n",
    "    ### Forward Pass ###\n",
    "    \n",
    "    # Schicht 1\n",
    "    Z1 = np.zeros(shape = (d, 4))    # Erstellen eines leeren Arrays zum Speichern von Z1 für jeden Datenpunkt\n",
    "    for n in range(d):\n",
    "        Z1[n] = z1(X0[n])    # Ermitteln von Z1 für jeden Datenpunkt\n",
    "    X1 = sig(Z1)    # Ermitteln von X1 für jeden Datenpunkt\n",
    "    \n",
    "    \n",
    "    # Schicht 2\n",
    "    Z2 = np.zeros(shape = (d, 2))    # Erstellen eines leeren Arrays zum Speichern von Z2 für jeden Datenpunkt\n",
    "    for n in range(d):\n",
    "        Z2[n] = z2(X1[n])    # Ermitteln von Z2 für jeden Datenpunkt\n",
    "    X2 = sig(Z2)    # Ermitteln von X2 für jeden Datenpunkt\n",
    "    \n",
    "    \n",
    "    # Schicht 3\n",
    "    Z3 = np.zeros(d)    # Erstellen eines leeren Arrays zum Speichern von Z3 für jeden Datenpunkt\n",
    "    for n in range(d):\n",
    "        Z3[n] = z3(X2[n])    # Ermitteln von Z3 für jeden Datenpunkt\n",
    "    X3 = sig(Z3)    # Ermitteln von X3 für jeden Datenpunkt\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### Backward Pass ###\n",
    "    \n",
    "    # Schicht 3\n",
    "    # Gradient des Verschiebungsvektors\n",
    "    dR_dw3V_temp = dR_dh(X3, y)*sig_prime(Z3)    # Bestimme einen Array mit den Gradienten der einzelnen Datenpunkten\n",
    "    dR_dw3V = np.sum(dR_dw3V_temp)    # Summiere die einzelnen Gradienten auf\n",
    "    \n",
    "    # Gradient der Matrix\n",
    "    dR_dw3M_temp = np.zeros(shape = (d, 2))    # Leerer Array zum Speichern des Gradienten für jeden Datenpunkt\n",
    "    for n in range(d):\n",
    "        dR_dw3M_temp[n] = dR_dw3V_temp[n]*X2[n]    # Ermitteln des Gradienten für jeden Datenpunkt\n",
    "    dR_dw3M = np.sum(dR_dw3M_temp, axis = 0)    # Summieren dier einzelnen Gradienten, axis = 0 summiert nur über Datenpunkte\n",
    "\n",
    "    \n",
    "    # Schicht 2\n",
    "    # Gradient des Verschiebungsvektors\n",
    "    dR_dw2V_temp = np.zeros(shape = (d, 2))    # Leerer Array zum Speichern des Gradienten für jeden Datenpunkt\n",
    "    for n in range(d):\n",
    "        dR_dw2V_temp[n] = dR_dw3V_temp[n]*w3M*sig_prime(Z2[n])    # Ermitteln des Gradienten für jeden Datenpunkt\n",
    "    dR_dw2V = np.sum(dR_dw2V_temp, axis = 0)    # Summieren dier einzelnen Gradienten, axis = 0 summiert nur über Datenpunkte\n",
    "\n",
    "    # Gradient der Matrix\n",
    "    dR_dw2M_temp = np.zeros(shape = (d, 2, 4))    # Leerer Array zum Speichern des Gradienten für jeden Datenpunkt\n",
    "    for n in range(d):\n",
    "        dR_dw2M_temp[n] = np.outer(dR_dw2V_temp[n], X1[n])    # Ermitteln des Gradienten für jeden Datenpunkt\n",
    "    dR_dw2M = np.sum(dR_dw2M_temp, axis = 0)    # Summieren dier einzelnen Gradienten, axis = 0 summiert nur über Datenpunkte\n",
    "\n",
    "    \n",
    "    # Schicht 1\n",
    "    # Gradient des Verschiebungsvektors\n",
    "    dR_dw1V_temp = np.zeros(shape = (d, 4))    # Leerer Array zum Speichern des Gradienten für jeden Datenpunkt\n",
    "    for n in range(d):\n",
    "        dR_dw1V_temp[n] = (dR_dw2V_temp[n].T@w2M)*sig_prime(Z1[n])    # Ermitteln des Gradienten für jeden Datenpunkt\n",
    "    dR_dw1V = np.sum(dR_dw1V_temp, axis = 0)    # Summieren dier einzelnen Gradienten, axis = 0 summiert nur über Datenpunkte\n",
    "\n",
    "    # Gradient der Matrix\n",
    "    dR_dw1M_temp = np.zeros(shape = (d, 4, 2))    # Leerer Array zum Speichern des Gradienten für jeden Datenpunkt\n",
    "    for n in range(d):\n",
    "        dR_dw1M_temp[n] = np.outer(dR_dw1V_temp[n], X0[n])    # Ermitteln des Gradienten für jeden Datenpunkt\n",
    "\n",
    "    dR_dw1M = np.sum(dR_dw1M_temp, axis = 0)    # Summieren dier einzelnen Gradienten, axis = 0 summiert nur über Datenpunkte\n",
    "    \n",
    "    \n",
    "    ### Updaten der Gewichte ###\n",
    "    \n",
    "    # Schicht 1\n",
    "    w1M = w1M-eta*dR_dw1M\n",
    "    w1V = w1V-eta*dR_dw1V\n",
    "    \n",
    "    # Schicht 2\n",
    "    w2M = w2M-eta*dR_dw2M\n",
    "    w2V = w2V-eta*dR_dw2V\n",
    "    \n",
    "    # Schicht 3\n",
    "    w3M = w3M-eta*dR_dw3M\n",
    "    w3V = w3V-eta*dR_dw3V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7641756b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Auftragen der Aktivierung in den einzelnen Schichten ###\n",
    "plt.figure(figsize = (20, 20))\n",
    "\n",
    "# Subplot für Eingabeschicht\n",
    "plt.subplot(441)\n",
    "\n",
    "plt.scatter(x1, x2, c = y, cmap = 'seismic')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, color = 'grey', linestyle = '--')\n",
    "\n",
    "\n",
    "# Subplot für Schicht 1, Neuron 1\n",
    "plt.subplot(442)\n",
    "\n",
    "h_eval = np.zeros(d)\n",
    "for n in range(d):\n",
    "    h_eval[n] = sig(z1(X0[n]))[0]\n",
    "\n",
    "plt.scatter(x1, x2, c = h_eval, cmap = 'seismic')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, color = 'grey', linestyle = '--')\n",
    "\n",
    "# Subplot für Schicht 1, Neuron 2\n",
    "plt.subplot(446)\n",
    "\n",
    "h_eval = np.zeros(d)\n",
    "for n in range(d):\n",
    "    h_eval[n] = sig(z1(X0[n]))[1]\n",
    "\n",
    "plt.scatter(x1, x2, c = h_eval, cmap = 'seismic')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, color = 'grey', linestyle = '--')\n",
    "\n",
    "# Subplot für Schicht 1, Neuron 3\n",
    "plt.subplot(4, 4, 10)\n",
    "\n",
    "h_eval = np.zeros(d)\n",
    "for n in range(d):\n",
    "    h_eval[n] = sig(z1(X0[n]))[2]\n",
    "\n",
    "plt.scatter(x1, x2, c = h_eval, cmap = 'seismic')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, color = 'grey', linestyle = '--')\n",
    "\n",
    "# Subplot für Schicht 1, Neuron 4\n",
    "plt.subplot(4, 4, 14)\n",
    "\n",
    "h_eval = np.zeros(d)\n",
    "for n in range(d):\n",
    "    h_eval[n] = sig(z1(X0[n]))[0]\n",
    "\n",
    "plt.scatter(x1, x2, c = h_eval, cmap = 'seismic')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, color = 'grey', linestyle = '--')\n",
    "\n",
    "# Subplot für Schicht 3, Neuron 1\n",
    "plt.subplot(443)\n",
    "\n",
    "h_eval = np.zeros(d)\n",
    "for n in range(d):\n",
    "    h_eval[n] = sig(z2(sig(z1(X0[n]))))[0]\n",
    "\n",
    "plt.scatter(x1, x2, c = h_eval, cmap = 'seismic')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, color = 'grey', linestyle = '--')\n",
    "\n",
    "# Subplot für Schicht 3, Neuron 2\n",
    "plt.subplot(447)\n",
    "\n",
    "h_eval = np.zeros(d)\n",
    "for n in range(d):\n",
    "    h_eval[n] = sig(z2(sig(z1(X0[n]))))[1]\n",
    "\n",
    "plt.scatter(x1, x2, c = h_eval, cmap = 'seismic')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, color = 'grey', linestyle = '--')\n",
    "\n",
    "# Subplot für Ausgabeschicht\n",
    "plt.subplot(444)\n",
    "\n",
    "h_eval = np.zeros(d)\n",
    "for n in range(d):\n",
    "    h_eval[n] = h(X0[n])\n",
    "\n",
    "plt.scatter(x1, x2, c = h_eval, cmap = 'seismic')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, color = 'grey', linestyle = '--')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9e7689",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Auftragung der Klassifikation des Netzes ###\n",
    "plt.figure(figsize = (5, 5))\n",
    "\n",
    "# Anlegen eines meshgrids\n",
    "N = 100\n",
    "X, Y = np.meshgrid(np.linspace(-1, 1, N), np.linspace(-1, 1, N))\n",
    "\n",
    "h_eval = np.zeros(shape = (N, N))    # Leerer Array zum Speiechern er Klassifikation durch das Netz\n",
    "    \n",
    "for n in range(N):\n",
    "    for m in range(N):\n",
    "        h_eval[n][m] = h(np.array([X[n][m], Y[n][m]]))    # Ermitteln der Klassifikation durch das Netz\n",
    "        \n",
    "# Auftragen der Klassifikation durch das Netz\n",
    "plt.contourf(X, Y, h_eval, cmap = 'seismic')\n",
    "\n",
    "# Auftragen der Datenpunkte\n",
    "plt.scatter(x1, x2, c = y , cmap = 'spring')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
